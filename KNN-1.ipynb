{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81cb83b-ac20-4f58-a00f-8cce37a772bf",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "Ans:-The K-Nearest Neighbors (KNN) algorithm is a simple and intuitive machine learning algorithm used for both classification and regression tasks. It is a non-parametric, instance-based, and lazy learning algorithm, meaning that it makes predictions based on the majority class or the average of the k-nearest data points in the feature space.\r\n",
    "\r\n",
    "Here's an overview of how the KNN algorithm works:\r\n",
    "\r\n",
    "Instance-Based Learning:\r\n",
    "\r\n",
    "KNN is an instance-based learning algorithm, meaning it does not explicitly learn a model during the training phase. Instead, it memorizes the entire training dataset.\r\n",
    "Distance Metric:\r\n",
    "\r\n",
    "KNN relies on a distance metric (usually Euclidean distance in most cases) to measure the similarity between data points. The choice of distance metric depends on the nature of the data.\r\n",
    "Parameter K:\r\n",
    "\r\n",
    "Thparameter \r\n",
    "�\r\n",
    "K is a positive integer representing the number of nearest neighbors to consider. It is a crucial parameter in the algorithm and is typically chosen based on cross-validation or other model evaluation techniques.\r\n",
    "Prediction for Classification:\r\n",
    "\r\n",
    "For a classification task, KNN assigns the class label that is most mmon among the \r\n",
    "�\r\n",
    "K nearest neighbors. The data point is assigned to the class that has the majori vote within its \r\n",
    "�\r\n",
    "K neighbors.\r\n",
    "Prediction for Regression:\r\n",
    "\r\n",
    "For a regression task, KNN predicts the target value by taking the average (or weighted average) of t target values of its \r\n",
    "�\r\n",
    "K nearest neighbors.\r\n",
    "Lazy Learning:\r\n",
    "\r\n",
    "KNN is considered a lazy learning algorithm because it postpones the learning process until the prediction phase. During pdiction, it identifies the \r\n",
    "�\r\n",
    "K nearest neighbors and makes predictions based on their values.\r\n",
    "Scalability:\r\n",
    "\r\n",
    "One limitation of KNN is its scalability, as it requires searching through the entire training dataset to find the nearest neighbors. Efficient data structures, such as KD-trees or Ball trees, can be used to speed up this process.\r\n",
    "Sensitive to Feature Scaling:\r\n",
    "\r\n",
    "KNN is sensitive to the scale of features, so it is often recommended to standardize or normalize the features before applying the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5ca2a-3eb8-4f20-add0-e3470705a1ba",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN\n",
    "Ans:-Choosing the value of �\r\n",
    "K in the K-Nearest Neighbors (KNN) algorithm is a critical decision that can significantly impact the performance of the model. The optima\r\n",
    "�\r\n",
    "K value depends on the characteristics of the dataset and the underlying patterns. Here are some common approaches to selecting the valuef \r\n",
    "�\r\n",
    "K:\r\n",
    "\r\n",
    "Grid Search with Cross-Validation:\r\n",
    "\r\n",
    "Perform a grid search over aange of \r\n",
    "�\r\n",
    "K values and use cross-validation to evaluate the performance of the mod for each \r\n",
    "�\r\n",
    " Choose the \r\n",
    "�\r\n",
    "K that provides the best performance. This method helps in preventing overfitting and finding a good balance between bias and variance.\r\n",
    "Odd Values for Binary Classification:\r\n",
    "\r\n",
    "For binary classification problems, it's common to choe odd values for \r\n",
    "�\r\n",
    "K to avoid ties in voting. Odd values help break ties, ensuring that the algorithm can reach a majority vote.\r\n",
    "Rule of Thumb:\r\n",
    "\r\n",
    "A simple le of thumb is to set \r\n",
    "�\r\n",
    "K to the square root of the number of samples in the training dataset. However, this is a heuristic and may not always be optimal. Adjustments might be needed based on the characteristics of the data.\r\n",
    "Elbow Method:\r\n",
    "\r\n",
    "Use the elbow method to find t point at which increasing \r\n",
    "�\r\n",
    "K does not lead to a significant improvement in model performance. Plot the performance metric (e.g accuracy) against different \r\n",
    "�\r\n",
    "K values and look for the point where the improvement starts to plateau.\r\n",
    "Domain Knowledge:\r\n",
    "\r\n",
    "Consider domain knowledge and the characteristics of the problem. Some datasets may have inherent structurethat suggest an optimal range for \r\n",
    "�\r\n",
    "K. For example, if asses are well-separated, a smaller \r\n",
    "�\r\n",
    "K might be appropriate.\r\n",
    "Expimentation:\r\n",
    "\r\n",
    "Experiment with different \r\n",
    "�\r\n",
    "K values and observe the model's performance on a validation set. Visulize the decision boundaries for different \r\n",
    "�\r\n",
    "K values to gain insights into how the model generalizes.\r\n",
    "Weighted Voting:\r\n",
    "\r\n",
    "Experiment with weighted voting. In KNN, assigning weights to the neighbors based on their distance can give more influence to closer neighbors. This can be especially useful when there is a varying density of data points across different regions of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a4390-aa2d-43d6-9219-af5883ba119b",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Ans:-The primary difference between the K-Nearest Neighbors (KNN) classifier and KNN regressor lies in their respective tasks—classification and regression. While both are based on the same underlying principles of finding the nearest neighbors in the feature space, they have distinct objectives:\r\n",
    "\r\n",
    "KNN Classifier:\r\n",
    "\r\n",
    "Task: The KNN classifier is used for classification tasks, where the goal is to predict the class or category of a data point based on its neighbors.\r\n",
    "Output: The output of a KNN classifier is the class label of the majority ofhe \r\n",
    "�\r\n",
    "K nearest neighbors. In binary classification, this could be a simple vote (e.if \r\n",
    "�\r\n",
    "=\r\n",
    "3\r\n",
    "K=3, the class with at least 2 votes wins). In multi-class classification, the class with the most votes is chosen.\r\n",
    "Example: Predicting whether an email is spam or not based on features like the frequency of certain words.\r\n",
    "KNN Regressor:\r\n",
    "\r\n",
    "Task: The KNN regressor is used for regression tasks, where the goal is to predict a continuous target variable based on the values of its neighbors.\r\n",
    "Output: The output of a KNN regressor is typically the average (or weighted average) of the target variablvalues of the \r\n",
    "�\r\n",
    "K nearest neighbors. This means the prediction is a continuous value.\r\n",
    "Example: Predicting the price of a house based on features like the number of bedrooms, square footage, etc.\r\n",
    "In summary, the main distinction is in the nature of the output:\r\n",
    "\r\n",
    "KNN Classifier: Discrete class labels (categories or classes).\r\n",
    "KNN Regressor: Continuous numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa27851-d904-467c-98c5-8f922239b397",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "Classification Metrics Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b041ced-9778-4d4e-a8be-fca946662983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_prob = knn_classifier.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9e016-4e32-42b0-9488-262a29d7145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Metrics Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab009316-f920-42e1-85ca-3e4434c7773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X_reg, y_reg = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_regressor.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = knn_regressor.predict(X_test_reg)\n",
    "\n",
    "# Evaluate regression metrics\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f04219-c7a2-4774-9277-96d940400ac7",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "Ans:-The \"curse of dimensionality\" refers to various challenges and issues that arise when working with high-dimensional data, and it particularly impacts algorithms like K-Nearest Neighbors (KNN). As the number of features or dimensions increases, the data becomes increasingly sparse, and the behavior of algorithms like KNN can be adversely affected. Here are some aspects of the curse of dimensionality in the context of KNN:\r\n",
    "\r\n",
    "Increased Computational Complexity:\r\n",
    "\r\n",
    "As the number of dimensions increases, the number of data points required to maintain the same level of data density also increases exponentially. This leads to higher computational costs when finding nearest neighbors in high-dimensional spaces.\r\n",
    "Distance Metric Distortion:\r\n",
    "\r\n",
    "In high-dimensional spaces, the concept of distance becomes less meaningful. The distance between points tends to become more uniform, and the differences in distances lose significance. This makes it challenging to identify meaningful neighbors using traditional distance metrics.\r\n",
    "Sparse Data:\r\n",
    "\r\n",
    "As the dimensionality increases, the available data becomes more sparse. In a high-dimensional space, most data points are far from each other, resulting in a situation where the nearest neighbors may not provide representative information about the local structure of the data.\r\n",
    "Overfitting:\r\n",
    "\r\n",
    "KNN is susceptible to overfitting in high-dimensional spaces. In lower-dimensional spaces, finding a few nearest neighbors can provide a good estimate of the local structure. However, in high-dimensional spaces, relying on a few neighbors may lead to overfitting, as the local structure becomes less well-defined.\r\n",
    "Increased Data Volume Requirements:\r\n",
    "\r\n",
    "To maintain the same level of representativeness in high-dimensional spaces, a significantly larger amount of data is required. Obtaining sufficient data becomes challenging, especially in domains where collecting data is expensive or time-consuming.\r\n",
    "Curse of Choice:\r\n",
    "\r\n",
    "The curse of dimensionality introduces challenges in choosing an apppriate value for \r\n",
    "�\r\n",
    "K (number of neighbs) in KNN. A small \r\n",
    "�\r\n",
    "K might lead to sensitivity tnoise, while a large \r\n",
    "�\r\n",
    "K might not capture the local structure effectively.\r\n",
    "Dimension Reduction Techniques:\r\n",
    "\r\n",
    "To address the curse of dimensionality, dimensionality reduction techniques (e.g., Principal Component Analysis, t-Distributed Stochastic Neighbor Embedding) are often employed to reduce the number of features while preserving important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2ed80-06ed-455b-9ecf-7f1830a69b66",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "Ans:-Handling missing values in the context of K-Nearest Neighbors (KNN) requires imputing or estimating the missing values to ensure that the algorithm can find meaningful neighbors. Here are several approaches to handle missing values in KNN:\r\n",
    "\r\n",
    "Imputation with Mean, Median, or Mode:\r\n",
    "\r\n",
    "Replace missing values with the mean, median, or mode of the feature. This is a simple imputation method and is suitable when the missing values are missing completely at random.\r\n",
    "Imputation using KNN Imputer:\r\n",
    "\r\n",
    "Sklearn provides a KNNImputer class that imputes missing values using the KNN algorithm. It considers other features and uses the nearest neighbors to estimate missing values. This method can handle both continuous and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b8335-5103-4a51-b86a-b4de73a2cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Instantiate the KNNImputer with the desired number of neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit and transform the data to impute missing values\n",
    "X_imputed = knn_imputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86e3a2-9e01-4d83-9cb7-d07d0690bbce",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "Ans:-The choice between using a K-Nearest Neighbors (KNN) classifier or regressor depends on the nature of the problem and the type of target variable you are trying to predict. Here's a comparison of the performance characteristics of KNN classifier and regressor:\r\n",
    "\r\n",
    "KNN Classifier:\r\n",
    "Task:\r\n",
    "\r\n",
    "Used for classification tasks where the goal is to predict the class or category of a data point based on its neighbors.\r\n",
    "Output:\r\n",
    "\r\n",
    "Provides discrete class labels. The predicted output is the majority class amg the \r\n",
    "�\r\n",
    "K nearest neighbors.\r\n",
    "Evaluation Metrics:\r\n",
    "\r\n",
    "Common classification metrics include accuracy, precision, recall, F1-score, confusion matrix, ROC curve, and AUC.\r\n",
    "Use Cases:\r\n",
    "\r\n",
    "Suitable for problems where the target variable is categorical or represents different classes. Examples include spam detection, image classification, and sentiment analysis.\r\n",
    "Considerations:\r\n",
    "\r\n",
    "Effective when the decision boundary is complex and non-linr. The choice of \r\n",
    "�\r\n",
    "K is crucial and may vary based on the characteristics of the dataset.\r\n",
    "KNN Regressor:\r\n",
    "Task:\r\n",
    "\r\n",
    "Used for regression tasks where the goal is to predict a continuous target variable based on the values of its neighbors.\r\n",
    "Output:\r\n",
    "\r\n",
    "Provides continuous numerical values. The predicted output is typically the average (or weighted average) of the taet variable values of the \r\n",
    "�\r\n",
    "K nearest neighbors.\r\n",
    "Evaluation Metrics:\r\n",
    "\r\n",
    "Common regression metrics include mean squared error (MSE), mean absolute error (MAE), R-squared, and explained variance score.\r\n",
    "Use Cases:\r\n",
    "\r\n",
    "Suitable for problems where the target variable is continuous and the prediction involves estimating a quantity. Examples include predicting house prices, temperature, or stock prices.\r\n",
    "Considerations:\r\n",
    "\r\n",
    "Effective when there is a correlation between the features and the target variable, and the relationship is not strictly linear. Similato the KNN classifier, the choice of \r\n",
    "�\r\n",
    "K is important and may require tuning.\r\n",
    "Comparison:\r\n",
    "Decision Boundary:\r\n",
    "\r\n",
    "KNN Classifier: Determines decision boundaries between different classes.\r\n",
    "KNN Regressor: Estimates a smooth continuous surface.\r\n",
    "Output Type:\r\n",
    "\r\n",
    "KNN Classifier: Discrete class labels.\r\n",
    "KNN Regressor: Continuous numerical values.\r\n",
    "Performance Metrics:\r\n",
    "\r\n",
    "KNN Classifier: Classification metrics.\r\n",
    "KNN Regressor: Regression metrics.\r\n",
    "Sensitivity to Noise:\r\n",
    "\r\n",
    "KNN Classifier: Sensitive to noise and outliers.\r\n",
    "KNN Regressor: May be less sensitive to noise due to averaging.\r\n",
    "Interpretability:\r\n",
    "\r\n",
    "KNN Classifier: Provides class labels.\r\n",
    "KNN Regressor: Provides numerical predictions.\r\n",
    "Choosing Between KNN Classifier and Regressor:\r\n",
    "Choose KNN Classifier when the target variable is categorical, and the goal is to classify data points into different classes.\r\n",
    "\r\n",
    "Choose KNN Regressor when the target variable is continuous, and the goal is to predict numerical values.\r\n",
    "\r\n",
    "Consider the characteristics of the problem, the nature of the data, and the desired output type when making a choice between classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b10d2d-45cc-4a2b-bd1c-9a93c8493a16",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "Ans:-Strengths of KNN:\r\n",
    "\r\n",
    "Simple and Intuitive:\r\n",
    "\r\n",
    "KNN is easy to understand and implement. It's a straightforward algorithm that doesn't require complex assumptions.\r\n",
    "Non-Parametric:\r\n",
    "\r\n",
    "Being a non-parametric algorithm, KNN does not make assumptions about the underlying distribution of the data. It can be effective in capturing complex relationships.\r\n",
    "Adaptability to Local Patterns:\r\n",
    "\r\n",
    "KNN adapts well to local patterns in the data and can be effective in capturing non-linear decision boundaries.\r\n",
    "No Training Phase:\r\n",
    "\r\n",
    "KNN has no explicit training phase. The model simply memorizes the training data, making it suitable for dynamic datasets.\r\n",
    "Versatility:\r\n",
    "\r\n",
    "KNN can be used for both classification and regression tasks, making it versatile for a range of problems.\r\n",
    "Weaknesses of KNN:\r\n",
    "\r\n",
    "Computational Complexity:\r\n",
    "\r\n",
    "Calculating distances between data points becomes computationally expensive as the size of the dataset and the number of dimensions increase. This is known as the curse of dimensionality.\r\n",
    "Sensitivity to Noise and Outliers:\r\n",
    "\r\n",
    "KNN is sensitive to noisy data and outliers. Outliers can significantly impact the decision boundaries and predictions.\r\n",
    "Need for Feature Scaling:\r\n",
    "\r\n",
    "KNN is sensitive to the scale of features. Features with larger scales may dominate the distance calculations.\r\n",
    "Parameter Sensitivity:\r\n",
    "\r\n",
    "The choiceof the number of neighbors (\r\n",
    "�\r\n",
    "K) can impact te performance of KNN. A small \r\n",
    "�\r\n",
    "K may ld to overfitting, while a large \r\n",
    "�\r\n",
    "K may smooth out local patterns.\r\n",
    "Imbalanced Data:\r\n",
    "\r\n",
    "KNN can be biased towards the majority class in imbalanced dasets, especially when using a small \r\n",
    "�\r\n",
    "K.\r\n",
    "Addressing Weaknesses:\r\n",
    "\r\n",
    "Feature Scaling:\r\n",
    "\r\n",
    "Normalize or standardize features to ensure equal importance in distance calculations.\r\n",
    "Dimensionality Reduction:\r\n",
    "\r\n",
    "Use dimensionality reduction techniques to reduce the number of features, especially in high-dimensional spaces.\r\n",
    "Outlier Handling:\r\n",
    "\r\n",
    "Identify and handle outliers before applying KNN. Robust distance metrics or outlier detection techniques can be employed.\r\n",
    "Cross-Validation:\r\n",
    "\r\n",
    "se cross-validation to tune hyperparameters, such as \r\n",
    "�\r\n",
    "K, and assess the generalization performance of the model.\r\n",
    "Distance Metrics:\r\n",
    "\r\n",
    "Experiment with different distance metrics based on the characteristics of the data. Euclidean distance is common, but other metrics (e.g., Manhattan, Minkowski) may be more suitable.\r\n",
    "Ensemble Methods:\r\n",
    "\r\n",
    "Consider ensemble methods like bagging or boosting to improve the robustness of KNN.\r\n",
    "Local Weighted Averaging:\r\n",
    "\r\n",
    "Introduce weighted averaging of neighbors, giving more importance to closer neighbors. This can reduce the impact of outliers.\r\n",
    "Data Preprocessing:\r\n",
    "\r\n",
    "Handle missing values appropriately and preprocess data to ensure its quality and reliability.\r\n",
    "Data Sampling:\r\n",
    "\r\n",
    "In the case of imbalanced datasets, consider techniques like oversampling or undersampling to address class imbalance.\r\n",
    "Use Approximate Nearest Neighbors:\r\n",
    "\r\n",
    "In situations with a large dataset, consider using approximate nearest neighbors algorithms to speed up the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ed557-3523-4dec-a331-4bcc5804ed24",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9ed7cf-eaa9-44d6-9d70-7ec7d5ab1910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two points in 2D space\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "\n",
    "# Euclidean distance calculation\n",
    "euclidean_distance = np.sqrt(np.sum((point1 - point2)**2))\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n",
    "\n",
    "# Manhattan distance calculation\n",
    "manhattan_distance = np.sum(np.abs(point1 - point2))\n",
    "print(\"Manhattan Distance:\", manhattan_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd77fc02-0c05-4c44-905b-5891cb3a28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "\n",
    "# Reshape points for compatibility with scikit-learn\n",
    "point1_reshaped = point1.reshape(1, -1)\n",
    "point2_reshaped = point2.reshape(1, -1)\n",
    "\n",
    "# Calculate distances using scikit-learn\n",
    "euclidean_distance_sklearn = euclidean_distances(point1_reshaped, point2_reshaped)[0][0]\n",
    "manhattan_distance_sklearn = manhattan_distances(point1_reshaped, point2_reshaped)[0][0]\n",
    "\n",
    "print(\"Euclidean Distance (scikit-learn):\", euclidean_distance_sklearn)\n",
    "print(\"Manhattan Distance (scikit-learn):\", manhattan_distance_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696faac1-720b-4d94-bfac-c628bb89cada",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
